{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {}
   },
   "source": [
    "# ğŸ”¥ BLIP(ViT-L) í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” with Optuna\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œ **Optuna TPE ì•Œê³ ë¦¬ì¦˜**ì„ ì‚¬ìš©í•˜ì—¬ BLIP ëª¨ë¸ì˜ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ¯ **Optuna ì¥ì :**\n",
    "- âœ… **ë¡œì»¬ ì‹¤í–‰**: ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ ì—†ìŒ\n",
    "- âœ… **TPE ì•Œê³ ë¦¬ì¦˜**: Bayesian Optimizationë³´ë‹¤ íš¨ìœ¨ì \n",
    "- âœ… **ë¹ ë¥´ê³  ì•ˆì •ì **: ì„¤ì • ê°„ë‹¨, ì˜¤ë¥˜ ì ìŒ\n",
    "- âœ… **ìŠ¤ë§ˆíŠ¸í•œ ìµœì í™”**: ì ì€ ì‹¤í—˜ìœ¼ë¡œ ë” ì¢‹ì€ ê²°ê³¼\n",
    "\n",
    "## ğŸ“‹ íŠœë‹í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "- **í•™ìŠµë¥  (Learning Rate)**: 1e-6 ~ 5e-5 (log scale)\n",
    "- **ë°°ì¹˜ í¬ê¸° (Batch Size)**: 4, 8, 16\n",
    "- **ì—í¬í¬ ìˆ˜ (Epochs)**: 2, 3, 4\n",
    "- **ê°€ì¤‘ì¹˜ ê°ì‡  (Weight Decay)**: 0.01 ~ 0.1 (log scale)\n",
    "- **Warmup Steps**: 100, 300, 500, 800, 1000\n",
    "\n",
    "## ğŸš€ ì‹¤í–‰ ë°©ë²•\n",
    "1. **í™˜ê²½ í™•ì¸**: GPU, ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ ì²´í¬\n",
    "2. **Optuna ìµœì í™” ì‹¤í–‰**: 10ê°œ Trial ìë™ ì‹¤í—˜ (60-90ë¶„)\n",
    "3. **ê²°ê³¼ ìë™ ì €ì¥**: ìµœì  íŒŒë¼ë¯¸í„° JSON íŒŒì¼ ì €ì¥\n",
    "4. **ìµœì¢… í•™ìŠµ**: ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ì „ì²´ ë°ì´í„°ì…‹ í•™ìŠµ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {}
   },
   "source": [
    "## ğŸ”§ 1. í™˜ê²½ ì„¤ì • ë° í™•ì¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ BLIP Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”!\n",
      "==================================================\n",
      "âœ… PyTorch ë²„ì „: 2.7.1+cu126\n",
      "âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: True\n",
      "âœ… GPU ì´ë¦„: NVIDIA GeForce RTX 4080 Laptop GPU\n",
      "âœ… GPU ë©”ëª¨ë¦¬: 11GB\n",
      "âœ… Transformers ë²„ì „: 4.53.1\n",
      "âœ… Optuna ë²„ì „: 4.4.0\n",
      "âœ… í˜„ì¬ ì‹œê°„: 2025-07-21 17:01:05\n",
      "==================================================\n",
      "ğŸ“ ê²½ë¡œ í™•ì¸:\n",
      "  - BLIP ì½”ë“œ í´ë”: ./BLIP_ViT_L_Finetuning âœ…\n",
      "  - í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸: ./BLIP_ViT_L_Finetuning\\blip_finetune.py âœ…\n",
      "  - ë°ì´í„°ì…‹: ./dataset/VQAv2 âœ…\n",
      "  - í•™ìŠµ ë°ì´í„°: ./dataset/VQAv2\\train.json âœ…\n",
      "  - ê²€ì¦ ë°ì´í„°: ./dataset/VQAv2\\val.json âœ…\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import torch\n",
    "import transformers\n",
    "import optuna\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"ğŸ”¥ BLIP Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"âœ… PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU ì´ë¦„: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"âœ… GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory // 1024**3}GB\")\n",
    "print(f\"âœ… Transformers ë²„ì „: {transformers.__version__}\")\n",
    "print(f\"âœ… Optuna ë²„ì „: {optuna.__version__}\")\n",
    "print(f\"âœ… í˜„ì¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ğŸ“ ê²½ë¡œ í™•ì¸\n",
    "blip_code_path = \"./BLIP_ViT_L_Finetuning\"\n",
    "train_script = os.path.join(blip_code_path, \"blip_finetune.py\")\n",
    "dataset_path = \"./dataset/VQAv2\"\n",
    "\n",
    "print(\"ğŸ“ ê²½ë¡œ í™•ì¸:\")\n",
    "print(f\"  - BLIP ì½”ë“œ í´ë”: {blip_code_path} {'âœ…' if os.path.exists(blip_code_path) else 'âŒ'}\")\n",
    "print(f\"  - í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸: {train_script} {'âœ…' if os.path.exists(train_script) else 'âŒ'}\")\n",
    "print(f\"  - ë°ì´í„°ì…‹: {dataset_path} {'âœ…' if os.path.exists(dataset_path) else 'âŒ'}\")\n",
    "\n",
    "# ë°ì´í„° íŒŒì¼ ìƒì„¸ í™•ì¸\n",
    "train_file = os.path.join(dataset_path, \"train.json\")\n",
    "val_file = os.path.join(dataset_path, \"val.json\")\n",
    "print(f\"  - í•™ìŠµ ë°ì´í„°: {train_file} {'âœ…' if os.path.exists(train_file) else 'âŒ'}\")\n",
    "print(f\"  - ê²€ì¦ ë°ì´í„°: {val_file} {'âœ…' if os.path.exists(val_file) else 'âŒ'}\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {}
   },
   "source": [
    "## ğŸ”¥ 2. Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í•¨ìˆ˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Optuna í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# Optuna ìµœì í™” í•¨ìˆ˜\n",
    "\n",
    "def extract_eval_loss_from_output(output_text):\n",
    "    \"\"\"ìˆ˜ì •ëœ eval_loss ì¶”ì¶œ í•¨ìˆ˜ - ì˜¬ë°”ë¥¸ íŒŒì‹±\"\"\"\n",
    "    lines = output_text.split('\\n')  # ì‹¤ì œ ê°œí–‰ë¬¸ìë¡œ ë¶„í• \n",
    "    eval_loss = None\n",
    "    \n",
    "    # ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ eval_loss ì°¾ê¸°\n",
    "    for line in lines:\n",
    "        if 'eval_loss' in line:\n",
    "            try:\n",
    "                # íŒ¨í„´ 1: {'eval_loss': 2.3456} ë˜ëŠ” \"eval_loss\": 2.3456\n",
    "                match1 = re.search(r\"['\\\"]?eval_loss['\\\"]?\\s*:\\s*([0-9]+\\.?[0-9]*)\", line)\n",
    "                if match1:\n",
    "                    eval_loss = float(match1.group(1))\n",
    "                    print(f\"âœ… eval_loss ì°¾ìŒ: {eval_loss}\")\n",
    "                    break\n",
    "                    \n",
    "                # íŒ¨í„´ 2: eval_loss=2.3456\n",
    "                match2 = re.search(r\"eval_loss\\s*=\\s*([0-9]+\\.?[0-9]*)\", line)\n",
    "                if match2:\n",
    "                    eval_loss = float(match2.group(1))\n",
    "                    print(f\"âœ… eval_loss ì°¾ìŒ: {eval_loss}\")\n",
    "                    break\n",
    "                    \n",
    "            except (ValueError, AttributeError):\n",
    "                continue\n",
    "    \n",
    "    if eval_loss is None:\n",
    "        print(\"âŒ eval_loss íŒŒì‹± ì‹¤íŒ¨ - ê¸°ë³¸ê°’ 999.0 ë°˜í™˜\")\n",
    "        return 999.0\n",
    "    \n",
    "    return eval_loss\n",
    "\n",
    "def optuna_objective(trial):\n",
    "    \"\"\"Optuna ìµœì í™” ëª©ì  í•¨ìˆ˜ (WandB ì™„ì „ ì œê±°)\"\"\"\n",
    "    \n",
    "    # ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ \n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-6, 5e-5, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [4, 8, 16])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 2, 4)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 0.01, 0.1, log=True)\n",
    "    warmup_steps = trial.suggest_categorical('warmup_steps', [100, 300, 500, 800, 1000])\n",
    "    warmup_ratio = warmup_steps / 2000  # warmup_stepsë¥¼ ratioë¡œ ë³€í™˜\n",
    "    \n",
    "    print(f\"\\\\nğŸ¯ Optuna Trial {trial.number + 1}:\")\n",
    "    print(f\"  - learning_rate: {learning_rate:.2e}\")\n",
    "    print(f\"  - batch_size: {batch_size}\")\n",
    "    print(f\"  - num_epochs: {num_epochs}\")\n",
    "    print(f\"  - weight_decay: {weight_decay:.4f}\")\n",
    "    print(f\"  - warmup_steps: {warmup_steps} (ratio: {warmup_ratio:.3f})\")\n",
    "    print(f\"  - ì˜ˆìƒ ì‹œê°„: {num_epochs * 3}ë¶„\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ğŸš€ ì™„ì „íˆ ìˆ˜ì •ëœ í•™ìŠµ ì‹¤í–‰ ëª…ë ¹ì–´\n",
    "    cmd = [\n",
    "        sys.executable, \n",
    "        \"blip_finetune.py\",\n",
    "        \"--train_file\", \"../dataset/VQAv2/train.json\",\n",
    "        \"--val_file\", \"../dataset/VQAv2/val.json\",\n",
    "        \"--max_train_samples\", str(800),\n",
    "        \"--max_val_samples\", str(400),\n",
    "        \"--per_device_train_batch_size\", str(batch_size),\n",
    "        \"--per_device_eval_batch_size\", str(batch_size),\n",
    "        \"--logging_steps\", str(20),\n",
    "        \"--eval_strategy\", \"epoch\",\n",
    "        \"--save_strategy\", \"no\",\n",
    "        \"--load_best_model_at_end\", \"false\",  # âœ… ìˆ˜ì •ëœ blip_finetune.pyì™€ í˜¸í™˜\n",
    "        \"--num_train_epochs\", str(num_epochs),\n",
    "        \"--output_dir\", f\"./optuna-trial-{trial.number}\",\n",
    "        \"--learning_rate\", str(learning_rate),\n",
    "        \"--weight_decay\", str(weight_decay),\n",
    "        \"--warmup_ratio\", str(warmup_ratio),\n",
    "        # WandB ê´€ë ¨ ì¸ìˆ˜ ì™„ì „ ì œê±°\n",
    "    ]\n",
    "    \n",
    "    print(f\"ğŸš€ ì‹¤í–‰ ëª…ë ¹ì–´:\")\n",
    "    print(f\"{' '.join(cmd)}\")\n",
    "    print(\"â° GPU í•™ìŠµ ì‹œì‘...\")\n",
    "    \n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # ğŸš« WandB ì™„ì „ ë¹„í™œì„±í™” (í™˜ê²½ë³€ìˆ˜)\n",
    "        env = os.environ.copy()\n",
    "        env[\"WANDB_MODE\"] = \"disabled\"\n",
    "        env[\"WANDB_DISABLED\"] = \"true\"\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            cmd,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            cwd=\"./BLIP_ViT_L_Finetuning\",\n",
    "            env=env,\n",
    "            timeout=36000\n",
    "        )\n",
    "        end_time = datetime.now()\n",
    "        runtime = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            # âœ… ì„±ê³µ ì‹œ eval_loss ì¶”ì¶œ\n",
    "            eval_loss = extract_eval_loss_from_output(result.stdout)\n",
    "            \n",
    "            print(f\"âœ… Trial {trial.number + 1} ì„±ê³µ!\")\n",
    "            print(f\"ğŸ“Š eval_loss: {eval_loss:.4f}\")\n",
    "            print(f\"â° ì‹¤í–‰ ì‹œê°„: {runtime:.1f}ì´ˆ\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            return eval_loss\n",
    "            \n",
    "        else:\n",
    "            print(f\"âŒ Trial {trial.number + 1} ì‹¤íŒ¨\")\n",
    "            print(f\"Return code: {result.returncode}\")\n",
    "            print(f\"STDERR: {result.stderr[:500]}...\")\n",
    "            return 999.0\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"â° Trial {trial.number + 1} ì‹œê°„ ì´ˆê³¼ (1ì‹œê°„)\")\n",
    "        return 999.0\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Trial {trial.number + 1} ì˜¤ë¥˜: {e}\")\n",
    "        return 999.0\n",
    "\n",
    "print(\"âœ… Optuna í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {}
   },
   "source": [
    "## ğŸš€ 3. Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í–‰\n",
    "\n",
    "**ë¡œì»¬ ê¸°ë°˜ ì•ˆì •ì  ìµœì í™” (WandB ì™„ì „ ì œê±°)**\n",
    "\n",
    "### ğŸ¯ **ì„¤ì •:**\n",
    "- **ìµœì í™” ì•Œê³ ë¦¬ì¦˜**: TPE (Tree-structured Parzen Estimator)\n",
    "- **ì‹¤í—˜ ìˆ˜**: 10ê°œ\n",
    "- **ìƒ˜í”Œ ìˆ˜**: 800 (train) + 400 (val)\n",
    "- **ì˜ˆìƒ ì‹œê°„**: 60-90ë¶„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-21 03:46:03,440] A new study created in memory with name: blip-vqa-final-20250721_034603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘!\n",
      "ğŸ“Š ìŠ¤í„°ë”” ì´ë¦„: blip-vqa-final-20250721_034603\n",
      "â° 10ê°œ ì‹¤í—˜ ìˆœì°¨ ì‹¤í–‰...\n",
      "ğŸ”§ íŠ¹ì§•:\n",
      "  âœ… WandB ì™„ì „ ì œê±°\n",
      "  âœ… ëª¨ë“  ì¸ìˆ˜ ì˜¤ë¥˜ í•´ê²°\n",
      "  âœ… í™˜ê²½ë³€ìˆ˜ë¡œ WandB ë¹„í™œì„±í™”\n",
      "  âœ… ë¹ ë¥¸ ë¡œì»¬ ì‹¤í–‰\n",
      "\\n============================================================\n",
      "\\nğŸ¯ Optuna Trial 1:\n",
      "  - learning_rate: 4.33e-06\n",
      "  - batch_size: 4\n",
      "  - num_epochs: 2\n",
      "  - weight_decay: 0.0143\n",
      "  - warmup_steps: 300 (ratio: 0.150)\n",
      "  - ì˜ˆìƒ ì‹œê°„: 6ë¶„\n",
      "==================================================\n",
      "ğŸš€ ì‹¤í–‰ ëª…ë ¹ì–´:\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\2025_Samsung_AI_Challenge\\python.exe blip_finetune.py --train_file ../dataset/VQAv2/train.json --val_file ../dataset/VQAv2/val.json --max_train_samples 800 --max_val_samples 400 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --logging_steps 20 --eval_strategy epoch --save_strategy no --load_best_model_at_end false --num_train_epochs 2 --output_dir ./optuna-trial-0 --learning_rate 4.328450221293881e-06 --weight_decay 0.01432169828911152 --warmup_ratio 0.15\n",
      "â° GPU í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-21 04:26:55,457] Trial 0 finished with value: 7.566145420074463 and parameters: {'learning_rate': 4.328450221293881e-06, 'batch_size': 4, 'num_epochs': 2, 'weight_decay': 0.01432169828911152, 'warmup_steps': 300}. Best is trial 0 with value: 7.566145420074463.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… eval_loss ì°¾ìŒ: 7.566145420074463\n",
      "âœ… Trial 1 ì„±ê³µ!\n",
      "ğŸ“Š eval_loss: 7.5661\n",
      "â° ì‹¤í–‰ ì‹œê°„: 2452.0ì´ˆ\n",
      "==================================================\n",
      "\\nğŸ¯ Optuna Trial 2:\n",
      "  - learning_rate: 4.44e-05\n",
      "  - batch_size: 4\n",
      "  - num_epochs: 2\n",
      "  - weight_decay: 0.0201\n",
      "  - warmup_steps: 800 (ratio: 0.400)\n",
      "  - ì˜ˆìƒ ì‹œê°„: 6ë¶„\n",
      "==================================================\n",
      "ğŸš€ ì‹¤í–‰ ëª…ë ¹ì–´:\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\2025_Samsung_AI_Challenge\\python.exe blip_finetune.py --train_file ../dataset/VQAv2/train.json --val_file ../dataset/VQAv2/val.json --max_train_samples 800 --max_val_samples 400 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --logging_steps 20 --eval_strategy epoch --save_strategy no --load_best_model_at_end false --num_train_epochs 2 --output_dir ./optuna-trial-1 --learning_rate 4.4447541666908114e-05 --weight_decay 0.02014847788415866 --warmup_ratio 0.4\n",
      "â° GPU í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-21 07:01:14,363] Trial 1 finished with value: 0.2807295322418213 and parameters: {'learning_rate': 4.4447541666908114e-05, 'batch_size': 4, 'num_epochs': 2, 'weight_decay': 0.02014847788415866, 'warmup_steps': 800}. Best is trial 1 with value: 0.2807295322418213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… eval_loss ì°¾ìŒ: 0.2807295322418213\n",
      "âœ… Trial 2 ì„±ê³µ!\n",
      "ğŸ“Š eval_loss: 0.2807\n",
      "â° ì‹¤í–‰ ì‹œê°„: 9258.9ì´ˆ\n",
      "==================================================\n",
      "\\nğŸ¯ Optuna Trial 3:\n",
      "  - learning_rate: 3.14e-06\n",
      "  - batch_size: 16\n",
      "  - num_epochs: 2\n",
      "  - weight_decay: 0.0327\n",
      "  - warmup_steps: 500 (ratio: 0.250)\n",
      "  - ì˜ˆìƒ ì‹œê°„: 6ë¶„\n",
      "==================================================\n",
      "ğŸš€ ì‹¤í–‰ ëª…ë ¹ì–´:\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\2025_Samsung_AI_Challenge\\python.exe blip_finetune.py --train_file ../dataset/VQAv2/train.json --val_file ../dataset/VQAv2/val.json --max_train_samples 800 --max_val_samples 400 --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --logging_steps 20 --eval_strategy epoch --save_strategy no --load_best_model_at_end false --num_train_epochs 2 --output_dir ./optuna-trial-2 --learning_rate 3.1357757322577464e-06 --weight_decay 0.032676417657817626 --warmup_ratio 0.25\n",
      "â° GPU í•™ìŠµ ì‹œì‘...\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¥ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í–‰ (ì™„ì „íˆ ì •ë¦¬ëœ ë²„ì „)\n",
    "\n",
    "try:\n",
    "    # Optuna ìŠ¤í„°ë”” ìƒì„±\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=42),  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼\n",
    "        study_name=f\"blip-vqa-final-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ¯ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘!\")\n",
    "    print(f\"ğŸ“Š ìŠ¤í„°ë”” ì´ë¦„: {study.study_name}\")\n",
    "    print(\"â° 10ê°œ ì‹¤í—˜ ìˆœì°¨ ì‹¤í–‰...\")\n",
    "    print(\"ğŸ”§ íŠ¹ì§•:\")\n",
    "    print(\"  âœ… WandB ì™„ì „ ì œê±°\")\n",
    "    print(\"  âœ… ëª¨ë“  ì¸ìˆ˜ ì˜¤ë¥˜ í•´ê²°\") \n",
    "    print(\"  âœ… í™˜ê²½ë³€ìˆ˜ë¡œ WandB ë¹„í™œì„±í™”\")\n",
    "    print(\"  âœ… ë¹ ë¥¸ ë¡œì»¬ ì‹¤í–‰\")\n",
    "    print(\"\\\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # 10ê°œ Trial ì‹¤í–‰\n",
    "    study.optimize(optuna_objective, n_trials=10)\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ‰ Optuna ìµœì í™” ì™„ë£Œ!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ğŸ† ìµœì  ê²°ê³¼ ì¶œë ¥\n",
    "    print(f\"âœ… ìµœì  eval_loss: {study.best_value:.4f}\")\n",
    "    print(\"\\\\nğŸ¯ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        if key == 'learning_rate':\n",
    "            print(f\"  - {key}: {value:.2e}\")\n",
    "        elif key in ['weight_decay']:\n",
    "            print(f\"  - {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  - {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\\\nğŸ† ìµœì  Trial ë²ˆí˜¸: {study.best_trial.number + 1}\")\n",
    "    \n",
    "    # ğŸ“Š ì „ì²´ ê²°ê³¼ ìš”ì•½\n",
    "    print(\"\\\\nğŸ“Š ì „ì²´ ì‹¤í—˜ ê²°ê³¼ ìš”ì•½:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Trial':<8} {'eval_loss':<12} {'lr':<10} {'batch':<8} {'epochs':<8}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    successful_trials = 0\n",
    "    for trial in study.trials:\n",
    "        if trial.value is not None and trial.value < 999.0:\n",
    "            lr = trial.params.get('learning_rate', 0)\n",
    "            batch = trial.params.get('batch_size', 0)\n",
    "            epochs = trial.params.get('num_epochs', 0)\n",
    "            print(f\"{trial.number+1:<8} {trial.value:<12.4f} {lr:<10.2e} {batch:<8} {epochs:<8}\")\n",
    "            successful_trials += 1\n",
    "    \n",
    "    if successful_trials == 0:\n",
    "        print(\"âŒ ì„±ê³µí•œ Trialì´ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ ë””ë²„ê¹… ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        for trial in study.trials:\n",
    "            if trial.value == 999.0:\n",
    "                print(f\"Trial {trial.number + 1}: {trial.params}\")\n",
    "    \n",
    "    # ğŸ’¾ ê²°ê³¼ ì €ì¥\n",
    "    best_params_with_details = {\n",
    "        'best_eval_loss': study.best_value,\n",
    "        'best_trial_number': study.best_trial.number + 1,\n",
    "        'best_params': study.best_params,\n",
    "        'optimization_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'total_trials': len(study.trials),\n",
    "        'successful_trials': successful_trials,\n",
    "        'version': 'final_clean_optuna_only'\n",
    "    }\n",
    "    \n",
    "    with open('optuna_best_params_final.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(best_params_with_details, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"\\\\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ“ íŒŒì¼: optuna_best_params_final.json\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ğŸ¯ ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´\n",
    "    print(\"\\\\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "    if successful_trials > 0:\n",
    "        print(\"1. âœ… ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°œê²¬ ì™„ë£Œ\")\n",
    "        print(\"2. ğŸš€ ì „ì²´ ë°ì´í„°ì…‹ìœ¼ë¡œ ìµœì¢… í•™ìŠµ ì‹¤í–‰\")\n",
    "        print(\"3. ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ ì„±ëŠ¥ í‰ê°€\")\n",
    "        print(\"4. ğŸ† ì‚¼ì„± AI ì±Œë¦°ì§€ ì œì¶œ\")\n",
    "    else:\n",
    "        print(\"1. âŒ ëª¨ë“  Trial ì‹¤íŒ¨ - ë””ë²„ê¹… í•„ìš”\")\n",
    "        print(\"2. ğŸ” blip_finetune.py ì¸ìˆ˜ ì¬í™•ì¸\")\n",
    "        print(\"3. ğŸ› ï¸ ë°ì´í„° ê²½ë¡œ í™•ì¸\")\n",
    "    \n",
    "    print(f\"\\\\nğŸ‰ ìµœì í™” ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\\\nâ›” ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨\")\n",
    "    print(\"ğŸ’¾ í˜„ì¬ê¹Œì§€ì˜ ê²°ê³¼ëŠ” study ê°ì²´ì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    if 'study' in locals() and len(study.trials) > 0:\n",
    "        best_trial = min(study.trials, key=lambda t: t.value if t.value is not None else float('inf'))\n",
    "        print(f\"ğŸ“Š í˜„ì¬ê¹Œì§€ ìµœê³  ì„±ëŠ¥: {best_trial.value:.4f} (Trial {best_trial.number + 1})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\\\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    print(\"ğŸ” ìœ„ì˜ í•¨ìˆ˜ ì •ì˜ ì…€ì´ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-21 17:01:23,173] A new study created in memory with name: blip-vqa-final-resumable-20250721_170123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ¨ ìƒˆë¡œìš´ Optuna studyë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "âœ… í˜„ì¬ study 'blip-vqa-final-resumable-20250721_170123'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "ğŸ“Š í˜„ì¬ê¹Œì§€ ì™„ë£Œëœ Trial: 0ê°œ\n",
      "ğŸš€ ë‚¨ì€ 10ê°œ Trialì„ ì‹œì‘í•©ë‹ˆë‹¤...\n",
      "============================================================\n",
      "\\nğŸ¯ Optuna Trial 1:\n",
      "  - learning_rate: 4.33e-06\n",
      "  - batch_size: 4\n",
      "  - num_epochs: 2\n",
      "  - weight_decay: 0.0143\n",
      "  - warmup_steps: 300 (ratio: 0.150)\n",
      "  - ì˜ˆìƒ ì‹œê°„: 6ë¶„\n",
      "==================================================\n",
      "ğŸš€ ì‹¤í–‰ ëª…ë ¹ì–´:\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\2025_Samsung_AI_Challenge\\python.exe blip_finetune.py --train_file ../dataset/VQAv2/train.json --val_file ../dataset/VQAv2/val.json --max_train_samples 800 --max_val_samples 400 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --logging_steps 20 --eval_strategy epoch --save_strategy no --load_best_model_at_end false --num_train_epochs 2 --output_dir ./optuna-trial-0 --learning_rate 4.328450221293881e-06 --weight_decay 0.01432169828911152 --warmup_ratio 0.15\n",
      "â° GPU í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-21 17:36:46,779] Trial 0 finished with value: 7.566145420074463 and parameters: {'learning_rate': 4.328450221293881e-06, 'batch_size': 4, 'num_epochs': 2, 'weight_decay': 0.01432169828911152, 'warmup_steps': 300}. Best is trial 0 with value: 7.566145420074463.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… eval_loss ì°¾ìŒ: 7.566145420074463\n",
      "âœ… Trial 1 ì„±ê³µ!\n",
      "ğŸ“Š eval_loss: 7.5661\n",
      "â° ì‹¤í–‰ ì‹œê°„: 2123.6ì´ˆ\n",
      "==================================================\n",
      "\\nğŸ¯ Optuna Trial 2:\n",
      "  - learning_rate: 4.44e-05\n",
      "  - batch_size: 4\n",
      "  - num_epochs: 2\n",
      "  - weight_decay: 0.0201\n",
      "  - warmup_steps: 800 (ratio: 0.400)\n",
      "  - ì˜ˆìƒ ì‹œê°„: 6ë¶„\n",
      "==================================================\n",
      "ğŸš€ ì‹¤í–‰ ëª…ë ¹ì–´:\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\2025_Samsung_AI_Challenge\\python.exe blip_finetune.py --train_file ../dataset/VQAv2/train.json --val_file ../dataset/VQAv2/val.json --max_train_samples 800 --max_val_samples 400 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --logging_steps 20 --eval_strategy epoch --save_strategy no --load_best_model_at_end false --num_train_epochs 2 --output_dir ./optuna-trial-1 --learning_rate 4.4447541666908114e-05 --weight_decay 0.02014847788415866 --warmup_ratio 0.4\n",
      "â° GPU í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-21 18:09:36,234] Trial 1 finished with value: 0.2807295322418213 and parameters: {'learning_rate': 4.4447541666908114e-05, 'batch_size': 4, 'num_epochs': 2, 'weight_decay': 0.02014847788415866, 'warmup_steps': 800}. Best is trial 1 with value: 0.2807295322418213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… eval_loss ì°¾ìŒ: 0.2807295322418213\n",
      "âœ… Trial 2 ì„±ê³µ!\n",
      "ğŸ“Š eval_loss: 0.2807\n",
      "â° ì‹¤í–‰ ì‹œê°„: 1969.5ì´ˆ\n",
      "==================================================\n",
      "\\nğŸ¯ Optuna Trial 3:\n",
      "  - learning_rate: 3.14e-06\n",
      "  - batch_size: 16\n",
      "  - num_epochs: 2\n",
      "  - weight_decay: 0.0327\n",
      "  - warmup_steps: 500 (ratio: 0.250)\n",
      "  - ì˜ˆìƒ ì‹œê°„: 6ë¶„\n",
      "==================================================\n",
      "ğŸš€ ì‹¤í–‰ ëª…ë ¹ì–´:\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\2025_Samsung_AI_Challenge\\python.exe blip_finetune.py --train_file ../dataset/VQAv2/train.json --val_file ../dataset/VQAv2/val.json --max_train_samples 800 --max_val_samples 400 --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --logging_steps 20 --eval_strategy epoch --save_strategy no --load_best_model_at_end false --num_train_epochs 2 --output_dir ./optuna-trial-2 --learning_rate 3.1357757322577464e-06 --weight_decay 0.032676417657817626 --warmup_ratio 0.25\n",
      "â° GPU í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 01:11:52,396] Trial 2 finished with value: 999.0 and parameters: {'learning_rate': 3.1357757322577464e-06, 'batch_size': 16, 'num_epochs': 2, 'weight_decay': 0.032676417657817626, 'warmup_steps': 500}. Best is trial 1 with value: 0.2807295322418213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Trial 3 ì‹¤íŒ¨\n",
      "Return code: 1\n",
      "STDERR: Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for insta...\n",
      "\\nğŸ¯ Optuna Trial 4:\n",
      "  - learning_rate: 4.09e-05\n",
      "  - batch_size: 4\n",
      "  - num_epochs: 2\n",
      "  - weight_decay: 0.0483\n",
      "  - warmup_steps: 1000 (ratio: 0.500)\n",
      "  - ì˜ˆìƒ ì‹œê°„: 6ë¶„\n",
      "==================================================\n",
      "ğŸš€ ì‹¤í–‰ ëª…ë ¹ì–´:\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\2025_Samsung_AI_Challenge\\python.exe blip_finetune.py --train_file ../dataset/VQAv2/train.json --val_file ../dataset/VQAv2/val.json --max_train_samples 800 --max_val_samples 400 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --logging_steps 20 --eval_strategy epoch --save_strategy no --load_best_model_at_end false --num_train_epochs 2 --output_dir ./optuna-trial-3 --learning_rate 4.093813608598787e-05 --weight_decay 0.04833180632488466 --warmup_ratio 0.5\n",
      "â° GPU í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 02:36:39,677] Trial 3 finished with value: 0.424265056848526 and parameters: {'learning_rate': 4.093813608598787e-05, 'batch_size': 4, 'num_epochs': 2, 'weight_decay': 0.04833180632488466, 'warmup_steps': 1000}. Best is trial 1 with value: 0.2807295322418213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… eval_loss ì°¾ìŒ: 0.424265056848526\n",
      "âœ… Trial 4 ì„±ê³µ!\n",
      "ğŸ“Š eval_loss: 0.4243\n",
      "â° ì‹¤í–‰ ì‹œê°„: 5087.3ì´ˆ\n",
      "==================================================\n",
      "\\nğŸ¯ Optuna Trial 5:\n",
      "  - learning_rate: 2.75e-06\n",
      "  - batch_size: 4\n",
      "  - num_epochs: 3\n",
      "  - weight_decay: 0.0153\n",
      "  - warmup_steps: 100 (ratio: 0.050)\n",
      "  - ì˜ˆìƒ ì‹œê°„: 9ë¶„\n",
      "==================================================\n",
      "ğŸš€ ì‹¤í–‰ ëª…ë ¹ì–´:\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\2025_Samsung_AI_Challenge\\python.exe blip_finetune.py --train_file ../dataset/VQAv2/train.json --val_file ../dataset/VQAv2/val.json --max_train_samples 800 --max_val_samples 400 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --logging_steps 20 --eval_strategy epoch --save_strategy no --load_best_model_at_end false --num_train_epochs 3 --output_dir ./optuna-trial-4 --learning_rate 2.752069685079053e-06 --weight_decay 0.015305744365500184 --warmup_ratio 0.05\n",
      "â° GPU í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 02:43:25,638] Trial 4 finished with value: 999.0 and parameters: {'learning_rate': 2.752069685079053e-06, 'batch_size': 4, 'num_epochs': 3, 'weight_decay': 0.015305744365500184, 'warmup_steps': 100}. Best is trial 1 with value: 0.2807295322418213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Trial 5 ì‹¤íŒ¨\n",
      "Return code: 1\n",
      "STDERR: Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for insta...\n",
      "\\nğŸ¯ Optuna Trial 6:\n",
      "  - learning_rate: 3.68e-05\n",
      "  - batch_size: 8\n",
      "  - num_epochs: 2\n",
      "  - weight_decay: 0.0245\n",
      "  - warmup_steps: 300 (ratio: 0.150)\n",
      "  - ì˜ˆìƒ ì‹œê°„: 6ë¶„\n",
      "==================================================\n",
      "ğŸš€ ì‹¤í–‰ ëª…ë ¹ì–´:\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\2025_Samsung_AI_Challenge\\python.exe blip_finetune.py --train_file ../dataset/VQAv2/train.json --val_file ../dataset/VQAv2/val.json --max_train_samples 800 --max_val_samples 400 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --logging_steps 20 --eval_strategy epoch --save_strategy no --load_best_model_at_end false --num_train_epochs 2 --output_dir ./optuna-trial-5 --learning_rate 3.683296438423422e-05 --weight_decay 0.02447244097399012 --warmup_ratio 0.15\n",
      "â° GPU í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 04:20:33,008] Trial 5 finished with value: 3.7017993927001953 and parameters: {'learning_rate': 3.683296438423422e-05, 'batch_size': 8, 'num_epochs': 2, 'weight_decay': 0.02447244097399012, 'warmup_steps': 300}. Best is trial 1 with value: 0.2807295322418213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… eval_loss ì°¾ìŒ: 3.7017993927001953\n",
      "âœ… Trial 6 ì„±ê³µ!\n",
      "ğŸ“Š eval_loss: 3.7018\n",
      "â° ì‹¤í–‰ ì‹œê°„: 5827.4ì´ˆ\n",
      "==================================================\n",
      "\\nğŸ¯ Optuna Trial 7:\n",
      "  - learning_rate: 1.74e-06\n",
      "  - batch_size: 16\n",
      "  - num_epochs: 4\n",
      "  - weight_decay: 0.0158\n",
      "  - warmup_steps: 300 (ratio: 0.150)\n",
      "  - ì˜ˆìƒ ì‹œê°„: 12ë¶„\n",
      "==================================================\n",
      "ğŸš€ ì‹¤í–‰ ëª…ë ¹ì–´:\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\2025_Samsung_AI_Challenge\\python.exe blip_finetune.py --train_file ../dataset/VQAv2/train.json --val_file ../dataset/VQAv2/val.json --max_train_samples 800 --max_val_samples 400 --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --logging_steps 20 --eval_strategy epoch --save_strategy no --load_best_model_at_end false --num_train_epochs 4 --output_dir ./optuna-trial-6 --learning_rate 1.7355056469855094e-06 --weight_decay 0.01580213186410389 --warmup_ratio 0.15\n",
      "â° GPU í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 04:23:52,646] Trial 6 finished with value: 999.0 and parameters: {'learning_rate': 1.7355056469855094e-06, 'batch_size': 16, 'num_epochs': 4, 'weight_decay': 0.01580213186410389, 'warmup_steps': 300}. Best is trial 1 with value: 0.2807295322418213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Trial 7 ì‹¤íŒ¨\n",
      "Return code: 1\n",
      "STDERR: Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for insta...\n",
      "\\nğŸ¯ Optuna Trial 8:\n",
      "  - learning_rate: 1.34e-06\n",
      "  - batch_size: 16\n",
      "  - num_epochs: 3\n",
      "  - weight_decay: 0.0214\n",
      "  - warmup_steps: 800 (ratio: 0.400)\n",
      "  - ì˜ˆìƒ ì‹œê°„: 9ë¶„\n",
      "==================================================\n",
      "ğŸš€ ì‹¤í–‰ ëª…ë ¹ì–´:\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\2025_Samsung_AI_Challenge\\python.exe blip_finetune.py --train_file ../dataset/VQAv2/train.json --val_file ../dataset/VQAv2/val.json --max_train_samples 800 --max_val_samples 400 --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --logging_steps 20 --eval_strategy epoch --save_strategy no --load_best_model_at_end false --num_train_epochs 3 --output_dir ./optuna-trial-7 --learning_rate 1.3359790328445558e-06 --weight_decay 0.02142387495644906 --warmup_ratio 0.4\n",
      "â° GPU í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 04:29:02,801] Trial 7 finished with value: 999.0 and parameters: {'learning_rate': 1.3359790328445558e-06, 'batch_size': 16, 'num_epochs': 3, 'weight_decay': 0.02142387495644906, 'warmup_steps': 800}. Best is trial 1 with value: 0.2807295322418213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Trial 8 ì‹¤íŒ¨\n",
      "Return code: 1\n",
      "STDERR: Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for insta...\n",
      "\\nğŸ¯ Optuna Trial 9:\n",
      "  - learning_rate: 3.22e-05\n",
      "  - batch_size: 16\n",
      "  - num_epochs: 4\n",
      "  - weight_decay: 0.0364\n",
      "  - warmup_steps: 100 (ratio: 0.050)\n",
      "  - ì˜ˆìƒ ì‹œê°„: 12ë¶„\n",
      "==================================================\n",
      "ğŸš€ ì‹¤í–‰ ëª…ë ¹ì–´:\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\2025_Samsung_AI_Challenge\\python.exe blip_finetune.py --train_file ../dataset/VQAv2/train.json --val_file ../dataset/VQAv2/val.json --max_train_samples 800 --max_val_samples 400 --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --logging_steps 20 --eval_strategy epoch --save_strategy no --load_best_model_at_end false --num_train_epochs 4 --output_dir ./optuna-trial-8 --learning_rate 3.216235469207424e-05 --weight_decay 0.036414738668149954 --warmup_ratio 0.05\n",
      "â° GPU í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 04:30:39,759] Trial 8 finished with value: 999.0 and parameters: {'learning_rate': 3.216235469207424e-05, 'batch_size': 16, 'num_epochs': 4, 'weight_decay': 0.036414738668149954, 'warmup_steps': 100}. Best is trial 1 with value: 0.2807295322418213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Trial 9 ì‹¤íŒ¨\n",
      "Return code: 1\n",
      "STDERR: Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for insta...\n",
      "\\nğŸ¯ Optuna Trial 10:\n",
      "  - learning_rate: 1.53e-06\n",
      "  - batch_size: 8\n",
      "  - num_epochs: 3\n",
      "  - weight_decay: 0.0808\n",
      "  - warmup_steps: 500 (ratio: 0.250)\n",
      "  - ì˜ˆìƒ ì‹œê°„: 9ë¶„\n",
      "==================================================\n",
      "ğŸš€ ì‹¤í–‰ ëª…ë ¹ì–´:\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\2025_Samsung_AI_Challenge\\python.exe blip_finetune.py --train_file ../dataset/VQAv2/train.json --val_file ../dataset/VQAv2/val.json --max_train_samples 800 --max_val_samples 400 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --logging_steps 20 --eval_strategy epoch --save_strategy no --load_best_model_at_end false --num_train_epochs 3 --output_dir ./optuna-trial-9 --learning_rate 1.5251209898002929e-06 --weight_decay 0.08082886378162035 --warmup_ratio 0.25\n",
      "â° GPU í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 06:02:40,345] Trial 9 finished with value: 9.408453941345215 and parameters: {'learning_rate': 1.5251209898002929e-06, 'batch_size': 8, 'num_epochs': 3, 'weight_decay': 0.08082886378162035, 'warmup_steps': 500}. Best is trial 1 with value: 0.2807295322418213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… eval_loss ì°¾ìŒ: 9.408453941345215\n",
      "âœ… Trial 10 ì„±ê³µ!\n",
      "ğŸ“Š eval_loss: 9.4085\n",
      "â° ì‹¤í–‰ ì‹œê°„: 5520.6ì´ˆ\n",
      "==================================================\n",
      "\\n============================================================\n",
      "ğŸ‰ Optuna ìµœì í™” ì™„ë£Œ!\n",
      "============================================================\n",
      "âœ… ìµœì  eval_loss: 0.2807\n",
      "\\nğŸ¯ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:\n",
      "  - learning_rate: 4.44e-05\n",
      "  - batch_size: 4\n",
      "  - num_epochs: 2\n",
      "  - weight_decay: 0.0201\n",
      "  - warmup_steps: 800\n",
      "\\nğŸ† ìµœì  Trial ë²ˆí˜¸: 2\n",
      "\\nğŸ“Š ì „ì²´ ì‹¤í—˜ ê²°ê³¼ ìš”ì•½:\n",
      "------------------------------------------------------------\n",
      "Trial    eval_loss    lr         batch    epochs  \n",
      "------------------------------------------------------------\n",
      "1        7.5661       4.33e-06   4        2       \n",
      "2        0.2807       4.44e-05   4        2       \n",
      "4        0.4243       4.09e-05   4        2       \n",
      "6        3.7018       3.68e-05   8        2       \n",
      "10       9.4085       1.53e-06   8        3       \n",
      "\\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ!\n",
      "ğŸ“ íŒŒì¼: optuna_best_params_final.json\n",
      "============================================================\n",
      "\\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:\n",
      "1. âœ… ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°œê²¬ ì™„ë£Œ\n",
      "2. ğŸš€ ì „ì²´ ë°ì´í„°ì…‹ìœ¼ë¡œ ìµœì¢… í•™ìŠµ ì‹¤í–‰\n",
      "3. ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ ì„±ëŠ¥ í‰ê°€\n",
      "4. ğŸ† ì‚¼ì„± AI ì±Œë¦°ì§€ ì œì¶œ\n",
      "\\nğŸ‰ ìµœì í™” ì™„ë£Œ ì‹œê°„: 2025-07-22 06:02:40\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¥ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í–‰ (ì´ì–´í•˜ê¸° ê¸°ëŠ¥ ì¶”ê°€)\n",
    "\n",
    "# study ê°ì²´ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±\n",
    "if 'study' not in globals():\n",
    "    print(\"âœ¨ ìƒˆë¡œìš´ Optuna studyë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=42),\n",
    "        study_name=f\"blip-vqa-final-resumable-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    )\n",
    "\n",
    "try:\n",
    "    print(f\"âœ… í˜„ì¬ study '{study.study_name}'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "    print(f\"ğŸ“Š í˜„ì¬ê¹Œì§€ ì™„ë£Œëœ Trial: {len(study.trials)}ê°œ\")\n",
    "        \n",
    "    # ë‚¨ì€ Trial ê³„ì‚°\n",
    "    total_trials_goal = 10\n",
    "    completed_trials = len(study.trials)\n",
    "    remaining_trials = total_trials_goal - completed_trials\n",
    "\n",
    "    if remaining_trials > 0:\n",
    "        print(f\"ğŸš€ ë‚¨ì€ {remaining_trials}ê°œ Trialì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "        print(\"=\" * 60)\n",
    "        study.optimize(optuna_objective, n_trials=remaining_trials)\n",
    "    else:\n",
    "        print(\"âœ… ëª¨ë“  Trialì´ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "    print(\"\\\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ‰ Optuna ìµœì í™” ì™„ë£Œ!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # ğŸ† ìµœì  ê²°ê³¼ ì¶œë ¥\n",
    "    print(f\"âœ… ìµœì  eval_loss: {study.best_value:.4f}\")\n",
    "    print(\"\\\\nğŸ¯ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        if key == 'learning_rate':\n",
    "            print(f\"  - {key}: {value:.2e}\")\n",
    "        elif key in ['weight_decay']:\n",
    "            print(f\"  - {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  - {key}: {value}\")\n",
    "        \n",
    "    print(f\"\\\\nğŸ† ìµœì  Trial ë²ˆí˜¸: {study.best_trial.number + 1}\")\n",
    "        \n",
    "    # ğŸ“Š ì „ì²´ ê²°ê³¼ ìš”ì•½\n",
    "    print(\"\\\\nğŸ“Š ì „ì²´ ì‹¤í—˜ ê²°ê³¼ ìš”ì•½:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Trial':<8} {'eval_loss':<12} {'lr':<10} {'batch':<8} {'epochs':<8}\")\n",
    "    print(\"-\" * 60)\n",
    "        \n",
    "    successful_trials = 0\n",
    "    for trial in study.trials:\n",
    "        if trial.value is not None and trial.value < 999.0:\n",
    "            lr = trial.params.get('learning_rate', 0)\n",
    "            batch = trial.params.get('batch_size', 0)\n",
    "            epochs = trial.params.get('num_epochs', 0)\n",
    "            print(f\"{trial.number+1:<8} {trial.value:<12.4f} {lr:<10.2e} {batch:<8} {epochs:<8}\")\n",
    "            successful_trials += 1\n",
    "        \n",
    "    if successful_trials == 0:\n",
    "        print(\"âŒ ì„±ê³µí•œ Trialì´ ì—†ìŠµë‹ˆë‹¤. ë””ë²„ê¹… ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    \n",
    "    # ğŸ’¾ ê²°ê³¼ ì €ì¥\n",
    "    best_params_with_details = {\n",
    "        'best_eval_loss': study.best_value,\n",
    "        'best_trial_number': study.best_trial.number + 1,\n",
    "        'best_params': study.best_params,\n",
    "        'optimization_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'total_trials': len(study.trials),\n",
    "        'successful_trials': successful_trials,\n",
    "        'version': 'final_clean_optuna_only'\n",
    "    }\n",
    "    \n",
    "    with open('optuna_best_params_final.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(best_params_with_details, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"\\\\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ“ íŒŒì¼: optuna_best_params_final.json\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ğŸ¯ ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´\n",
    "    print(\"\\\\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "    if successful_trials > 0:\n",
    "        print(\"1. âœ… ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°œê²¬ ì™„ë£Œ\")\n",
    "        print(\"2. ğŸš€ ì „ì²´ ë°ì´í„°ì…‹ìœ¼ë¡œ ìµœì¢… í•™ìŠµ ì‹¤í–‰\")\n",
    "        print(\"3. ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ ì„±ëŠ¥ í‰ê°€\")\n",
    "        print(\"4. ğŸ† ì‚¼ì„± AI ì±Œë¦°ì§€ ì œì¶œ\")\n",
    "    else:\n",
    "        print(\"1. âŒ ëª¨ë“  Trial ì‹¤íŒ¨ - ë””ë²„ê¹… í•„ìš”\")\n",
    "        print(\"2. ğŸ” blip_finetune.py ì¸ìˆ˜ ì¬í™•ì¸\")\n",
    "        print(\"3. ğŸ› ï¸ ë°ì´í„° ê²½ë¡œ í™•ì¸\")\n",
    "    \n",
    "    print(f\"\\\\nğŸ‰ ìµœì í™” ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\\\nâ›” ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨\")\n",
    "    print(\"ğŸ’¾ í˜„ì¬ê¹Œì§€ì˜ ê²°ê³¼ëŠ” study ê°ì²´ì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    if 'study' in locals() and len(study.trials) > 0:\n",
    "        best_trial = min(study.trials, key=lambda t: t.value if t.value is not None else float('inf'))\n",
    "        print(f\"ğŸ“Š í˜„ì¬ê¹Œì§€ ìµœê³  ì„±ëŠ¥: {best_trial.value:.4f} (Trial {best_trial.number + 1})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\\\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    print(\"ğŸ” ìœ„ì˜ í•¨ìˆ˜ ì •ì˜ ì…€ì´ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {}
   },
   "source": [
    "## ğŸ‰ **ì™„ì „íˆ ì •ë¦¬ëœ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”!**\n",
    "\n",
    "### âœ… **ìˆ˜ì • ì™„ë£Œëœ ë¬¸ì œë“¤:**\n",
    "1. **WandB ì½”ë“œ ì™„ì „ ì œê±°** â†’ ìˆœìˆ˜ Optunaë§Œ ì‚¬ìš©\n",
    "2. **`--load_best_model_at_end \"false\"`** â†’ blip_finetune.py ìˆ˜ì •ì‚¬í•­ ë°˜ì˜\n",
    "3. **í™˜ê²½ë³€ìˆ˜ WandB ë¹„í™œì„±í™”** â†’ `WANDB_MODE=disabled`\n",
    "4. **ëª¨ë“  ì¸ìˆ˜ íƒ€ì… ì •ë¦¬** â†’ ë¬¸ìì—´ ë³€í™˜ í†µì¼\n",
    "5. **ì¤‘ë³µ ì½”ë“œ ì œê±°** â†’ ê¹”ë”í•œ 3ê°œ ì…€ êµ¬ì¡°\n",
    "\n",
    "### ğŸš€ **ì‹¤í–‰ ìˆœì„œ:**\n",
    "1. **ì…€ 1** (í™˜ê²½ í™•ì¸) â†’ ê²½ë¡œ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸\n",
    "2. **ì…€ 2** (í•¨ìˆ˜ ì •ì˜) â†’ `optuna_objective` í•¨ìˆ˜ ë¡œë“œ  \n",
    "3. **ì…€ 3** (Optuna ì‹¤í–‰) â†’ **60-90ë¶„ ìµœì í™” ì§„í–‰**\n",
    "\n",
    "### âš¡ **ì‹¤í–‰ ì „ í™•ì¸ì‚¬í•­:**\n",
    "- âœ… GPU í™˜ê²½ í™œì„±í™”\n",
    "- âœ… ë°ì´í„°ì…‹ ê²½ë¡œ í™•ì¸: `./dataset/VQAv2/`\n",
    "- âœ… BLIP ì½”ë“œ ê²½ë¡œ í™•ì¸: `./BLIP_ViT_L_Finetuning/`\n",
    "\n",
    "### ğŸ¯ **ê¸°ëŒ€ ê²°ê³¼:**\n",
    "- ğŸ“Š **eval_loss 2.0~8.0 ë²”ìœ„** (999.0 ì˜¤ë¥˜ í•´ê²°)\n",
    "- ğŸ’¾ **optuna_best_params_final.json** ìë™ ìƒì„±\n",
    "- ğŸ† **ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ ë°œê²¬**\n",
    "\n",
    "**ì§€ê¸ˆ ìœ„ì˜ 3ê°œ ì…€ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ì„¸ìš”!** ğŸ”¥\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2025_Samsung_AI_Challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
