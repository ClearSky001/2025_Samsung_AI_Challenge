# BLIP VQAv2 Fine-tuning with WandB

BLIP(ViT-L) ëª¨ë¸ì„ VQAv2 ë°ì´í„°ì…‹ìœ¼ë¡œ fine-tuningí•˜ê³  WandBë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì§€ì›í•˜ëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
2025_Samsung_AI_Challenge/
â”œâ”€â”€ vqav2_dataset.py       # VQAv2 ë°ì´í„°ì…‹ ë¡œë”
â”œâ”€â”€ blip_finetune.py       # ë©”ì¸ fine-tuning ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ sweep_config.yaml      # WandB sweep ì„¤ì •
â”œâ”€â”€ run_sweep.py          # Sweep ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements.txt      # í•„ìš”í•œ íŒ¨í‚¤ì§€
â”œâ”€â”€ README.md            # ì‚¬ìš©ë²• ì„¤ëª…
â”œâ”€â”€ preprocess_vqav2.py  # ë°ì´í„° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ dataset/
    â””â”€â”€ VQAv2/
        â”œâ”€â”€ train.json   # ì „ì²˜ë¦¬ëœ í•™ìŠµ ë°ì´í„°
        â””â”€â”€ val.json     # ì „ì²˜ë¦¬ëœ ê²€ì¦ ë°ì´í„°
```

## ğŸš€ ì„¤ì¹˜ ë° ì„¤ì •

### 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
pip install -r requirements.txt
```

### 2. WandB ì„¤ì •
```bash
wandb login
```

## ğŸ’» ì‚¬ìš©ë²•

### 1. ì „ì²˜ë¦¬ ì‹¤í–‰
```bash
python preprocess_vqav2.py --base_dir ../dataset/VQAv2
```

### 2. ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©)
```bash
# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì†Œê·œëª¨ ë°ì´í„°ì…‹
python run_sweep.py single --max_train_samples 1000 --max_val_samples 500

# ì „ì²´ ë°ì´í„°ì…‹ìœ¼ë¡œ ë‹¨ì¼ ì‹¤í—˜
python blip_finetune.py --num_train_epochs 3 --per_device_train_batch_size 16
```

### 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° Sweep ì‹¤í–‰
```bash
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ 20ê°œ ì‹¤í—˜ ì‹¤í–‰
python run_sweep.py --count 20

# ì»¤ìŠ¤í…€ í”„ë¡œì íŠ¸ëª…ìœ¼ë¡œ ì‹¤í–‰
python run_sweep.py --project "my-blip-experiment" --count 10
```

### 4. ì»¤ìŠ¤í…€ íŒŒë¼ë¯¸í„°ë¡œ ì‹¤í—˜
```bash
python blip_finetune.py \
    --learning_rate 3e-5 \
    --num_train_epochs 5 \
    --per_device_train_batch_size 32 \
    --weight_decay 0.05 \
    --wandb_name "custom-experiment"
```

## âš™ï¸ ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°

### í•™ìŠµ ê´€ë ¨
- `learning_rate`: í•™ìŠµë¥  (ê¸°ë³¸ê°’: 2e-5)
- `num_train_epochs`: í•™ìŠµ ì—í¬í¬ ìˆ˜ (ê¸°ë³¸ê°’: 3)
- `per_device_train_batch_size`: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 16)
- `weight_decay`: ê°€ì¤‘ì¹˜ ê°ì‡  (ê¸°ë³¸ê°’: 0.01)
- `warmup_ratio`: ì›Œë°ì—… ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.1)

### ë°ì´í„° ê´€ë ¨
- `train_data_path`: ì „ì²˜ë¦¬ëœ train JSON ê²½ë¡œ (ê¸°ë³¸ê°’: ../dataset/VQAv2/train.json)
- `val_data_path`: ì „ì²˜ë¦¬ëœ val JSON ê²½ë¡œ (ê¸°ë³¸ê°’: ../dataset/VQAv2/val.json)
- `max_train_samples`: ìµœëŒ€ í•™ìŠµ ìƒ˜í”Œ ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)
- `max_val_samples`: ìµœëŒ€ ê²€ì¦ ìƒ˜í”Œ ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)
- `max_length`: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸ê°’: 512)

### Optuna ê´€ë ¨
- `best_params_path`: Optuna ê²°ê³¼ JSON ê²½ë¡œ (ê¸°ë³¸ê°’: ../optuna_best_params_final.json)

### WandB ê´€ë ¨
- `wandb_project`: WandB í”„ë¡œì íŠ¸ëª…
- `wandb_name`: ì‹¤í—˜ ì´ë¦„

## ğŸ“Š WandB Sweep ì„¤ì •

`sweep_config.yaml`ì—ì„œ ë‹¤ìŒ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì´ ìë™ìœ¼ë¡œ íŠœë‹ë©ë‹ˆë‹¤:

- **Learning Rate**: 1e-6 ~ 1e-4 (log-uniform)
- **Batch Size**: [8, 16, 32]
- **Epochs**: [3, 5, 8]
- **Weight Decay**: 0.01 ~ 0.3 (uniform)
- **Warmup Ratio**: 0.05 ~ 0.3 (uniform)
- **LR Scheduler**: ["linear", "cosine", "polynomial", "constant_with_warmup"]

## ğŸ¯ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

WandBë¥¼ í†µí•´ ë‹¤ìŒ ë©”íŠ¸ë¦­ë“¤ì„ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- **Training Loss**: í•™ìŠµ ì†ì‹¤
- **Evaluation Accuracy**: ê²€ì¦ ì •í™•ë„
- **Learning Rate**: ì‹¤ì‹œê°„ í•™ìŠµë¥  ë³€í™”
- **GPU Memory**: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

## ğŸ’¾ ëª¨ë¸ ì €ì¥

- ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì´ ìë™ìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤
- WandB Artifactsë¡œ ëª¨ë¸ ë²„ì „ ê´€ë¦¬
- ë¡œì»¬ì— ì²´í¬í¬ì¸íŠ¸ ì €ì¥

## ğŸ”§ ë¬¸ì œ í•´ê²°

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
python blip_finetune.py --per_device_train_batch_size 8

# ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ì‚¬ìš©
python blip_finetune.py --gradient_accumulation_steps 2
```

### ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
```bash
# ì†Œê·œëª¨ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
python blip_finetune.py --max_train_samples 100 --max_val_samples 50 --num_train_epochs 1
```

## ğŸ“ˆ ê²°ê³¼ í™•ì¸

1. **WandB Dashboard**: ì‹¤ì‹œê°„ í•™ìŠµ ì§„í–‰ ìƒí™©
2. **ë¡œì»¬ ì €ì¥**: `./blip-vqa-finetuned/` í´ë”
3. **ìµœì¢… í‰ê°€**: ì½˜ì†”ì— ì¶œë ¥ë˜ëŠ” ìµœì¢… ê²°ê³¼

## ğŸ‰ ì™„ë£Œ í›„

Fine-tuningì´ ì™„ë£Œë˜ë©´:
1. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì´ ì €ì¥ë©ë‹ˆë‹¤
2. WandBì—ì„œ ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ë¥¼ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
3. ì €ì¥ëœ ëª¨ë¸ë¡œ inferenceë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

---

## ğŸ“ ì˜ˆì‹œ ëª…ë ¹ì–´

```bash
# 1. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
python run_sweep.py single

# 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
python run_sweep.py --count 15

# 3. ì „ì²´ ë°ì´í„°ì…‹ í•™ìŠµ
python blip_finetune.py --num_train_epochs 5

# 4. ì»¤ìŠ¤í…€ ì‹¤í—˜
python blip_finetune.py --learning_rate 5e-5 --wandb_name "high-lr-experiment"
``` 